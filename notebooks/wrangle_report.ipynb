{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report WeRatesDogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "*\"A huge amount of effort is spent cleaning data to get it ready for analysis\"*, H. Wickham.\n",
    "\n",
    "The following document will detail a data wrangling approach on a twitter dataset, as part of an assignment for the Udacity Data Analyst Nanodegree. More details can be found on the main github repository page.\n",
    "\n",
    "*\"This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets.\"*, H. Wickham.\n",
    "\n",
    "[Tidy Data, H Wickham, published on the Journal of Statistical Software](#https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather\n",
    "\n",
    "Depending on the source of your data, and what format it's in, the steps in gathering data vary.\n",
    "High-level gathering process: obtaining data (downloading a file from the internet, scraping a web page, querying an API, etc.) and importing that data into your programming environment (e.g., Jupyter Notebook).\n",
    "\n",
    "In this assignement, the data gathered is a follow:\n",
    "\n",
    "- **The WeRateDogs Twitter archive**. Download this file manually by clicking the following link: [twitter_archive_enhanced.csv](#https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv).\n",
    "\n",
    "- **The tweet image predictions**, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the following [URL](#https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv).\n",
    "\n",
    "- **Twitter API call**,each tweet's retweet count and favorite (\"like\") count at minimum, and any additional data you find interesting. Using the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess\n",
    "\n",
    "**Assess data for:**\n",
    "- Quality: issues with content. Low quality data is also known as dirty data.\n",
    "- Tidiness: issues with structure that prevent easy analysis. Untidy data is also known as messy data. Tidy data requirements:\n",
    "    - Each variable forms a column.\n",
    "    - Each observation forms a row.\n",
    "    - Each type of observational unit forms a table.\n",
    "\n",
    "**Types of assessment:**\n",
    "- Visual assessment: scrolling through the data in your preferred software application (Google Sheets, Excel, a text editor, etc.).\n",
    "- Programmatic assessment: using code to view specific portions and summaries of the data (pandas' head, tail, and info methods, for example).\n",
    "\n",
    "In this assignement, the data issues identified after the assessment are as follow:\n",
    "\n",
    "**Quality**, *issues with content. Low quality data is also known as dirty data.*\n",
    "- Drop unused columns\n",
    "- Drop retweeted rows\n",
    "- Drop rows without image classifier outputs nor twitter API call outputs\n",
    "- Fix the dog name extracts\n",
    "- Clean the source field\n",
    "- Clean the rating_numerators and rating_denominators\n",
    "- Correct the data types\n",
    "- Unpivot the dog stages into one column\n",
    "- Convert the value \"None\" to numpy.nan\n",
    "\n",
    "\n",
    "**Tidiness**, *issues with structure that prevent easy analysis. Untidy data is also known as messy data.*\n",
    "- Drop unused columns in twitter_archive and in image_classifier_output\n",
    "- Merge twitter_archive, image_classifier_output and json_tweets into one dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "**Types of cleaning:**\n",
    "- Manual (not recommended unless the issues are single occurrences)\n",
    "- Programmatic\n",
    "**The programmatic data cleaning process:**\n",
    "- Define: convert our assessments into defined cleaning tasks. These definitions also serve as an instruction list so others (or yourself in the future) can look at your work and reproduce it.\n",
    "- Code: convert those definitions to code and run that code.\n",
    "- Test: test your dataset, visually or with code, to make sure your cleaning operations worked.\n",
    "    Always make copies of the original pieces of data before cleaning!\n",
    "\n",
    "**Reassess and Iterate**\n",
    "- After cleaning, always reassess and iterate on any of the data wrangling steps if necessary.\n",
    "\n",
    "**In this assignement, the data cleaning has involved:**\n",
    "- Extensive usage of built in methods within the Pandas package\n",
    "- For loops\n",
    "- Regular Expressions\n",
    "- Merge\n",
    "- Melt (unpivot)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
